import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px

st.set_page_config(page_title="æµç¨‹è¯„åˆ†Demo", layout="wide")
st.title("ğŸ“Š è‡ªåŠ¨æµç¨‹è¯„åˆ†åŸå‹ç³»ç»Ÿ")

# ä¸Šä¼ CSVæ–‡ä»¶
st.subheader("ğŸ“¤ ä¸Šä¼ CSVæ–‡ä»¶è¿›è¡Œè¯„åˆ†")
uploaded_file = st.file_uploader("ä¸Šä¼ åŒ…å«æµç¨‹æ—¥å¿—çš„CSVæ–‡ä»¶", type="csv")

default_fields = {
    'process_id': 'æµç¨‹ID',
    'event_id': 'äº‹ä»¶ID',
    'activity_name': 'äº‹ä»¶åç§°',
    'start_time': 'äº‹ä»¶å¼€å§‹æ—¶é—´',
    'end_time': 'äº‹ä»¶ç»“æŸæ—¶é—´',
    'performer': 'äº‹ä»¶æ‰§è¡Œäºº',
    'org_unit': 'äº‹ä»¶æ‰§è¡Œæœºæ„',
    'status': 'æµç¨‹çŠ¶æ€'
}

field_mapping = {}

df = None
if uploaded_file is not None:
    raw_df = pd.read_csv(uploaded_file)
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ£€æµ‹åˆ°ä»¥ä¸‹å­—æ®µï¼š")
    st.write(list(raw_df.columns))

    st.subheader("ğŸ› ï¸ å­—æ®µæ˜ å°„é…ç½®")
    for key, label in default_fields.items():
        options = [col for col in raw_df.columns]
        selected = st.selectbox(f"é€‰æ‹©å¯¹åº”çš„ã€{label}ã€‘å­—æ®µ:", options, key=key)
        field_mapping[key] = selected

    try:
        df = raw_df.rename(columns=field_mapping)
        df['start_time'] = pd.to_datetime(df['start_time'])
        df['end_time'] = pd.to_datetime(df['end_time'])
        st.success("âœ… å­—æ®µæ˜ å°„æˆåŠŸï¼Œå·²è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼")
    except Exception as e:
        st.error(f"âŒ å­—æ®µè½¬æ¢å‡ºé”™: {e}")
        st.stop()
else:
    st.info("ğŸ“¥ è¯·ä¸Šä¼ åŒ…å«æµç¨‹æ—¥å¿—çš„CSVæ–‡ä»¶...")

if df is None:
    st.subheader("ğŸ§ª æœªä¸Šä¼ æ•°æ®ï¼Œä½¿ç”¨å†…ç½®æ¨¡æ‹Ÿæ•°æ®")
    def generate_process_data():
        np.random.seed(42)
        process_ids = [f"P{str(i).zfill(3)}" for i in range(1, 6)]
        events_per_process = [3, 4, 5, 3, 4]
        org_units = ['BranchA', 'BranchB', 'HQ']
        activities = ['Receive Request', 'Review', 'Approve', 'Finalize', 'Archive']
        statuses = ['success', 'failed']

        rows = []
        start_base = datetime(2025, 8, 1, 9, 0, 0)

        for pid, num_events in zip(process_ids, events_per_process):
            current_time = start_base + timedelta(minutes=np.random.randint(0, 30))
            for eid in range(num_events):
                event_id = f"E{eid+1:02d}"
                activity = activities[eid % len(activities)]
                duration = timedelta(minutes=np.random.randint(5, 30))
                start_time = current_time
                end_time = start_time + duration
                performer = f"User{np.random.choice(list('ABCDE'))}"
                org_unit = np.random.choice(org_units)
                status = np.random.choice(statuses if eid == num_events - 1 else ['success'])
                rows.append([pid, event_id, activity, start_time, end_time, performer, org_unit, status])
                current_time = end_time + timedelta(minutes=np.random.randint(1, 10))

        df = pd.DataFrame(rows, columns=[
            'process_id', 'event_id', 'activity_name', 'start_time', 'end_time', 'performer', 'org_unit', 'status'
        ])
        return df

    df = generate_process_data()

# è®¡ç®—æµç¨‹æŒ‡æ ‡
def compute_metrics(df):
    process_metrics = []

    for pid, group in df.groupby("process_id"):
        group_sorted = group.sort_values("start_time")
        total_duration = (group_sorted["end_time"].max() - group_sorted["start_time"].min()).total_seconds() / 60.0
        avg_activity_duration = (group_sorted["end_time"] - group_sorted["start_time"]).dt.total_seconds().mean() / 60.0
        num_activities = group_sorted.shape[0]
        org_changes = group_sorted["org_unit"].nunique()
        failed_steps = group_sorted[group_sorted["status"] == "failed"].shape[0]
        wait_times = (group_sorted["start_time"].iloc[1:].reset_index(drop=True) - 
                      group_sorted["end_time"].iloc[:-1].reset_index(drop=True)).dt.total_seconds() / 60.0
        avg_wait_time = wait_times.mean() if not wait_times.empty else 0.0

        process_metrics.append({
            "process_id": pid,
            "total_duration_min": total_duration,
            "avg_activity_duration_min": avg_activity_duration,
            "num_activities": num_activities,
            "num_org_units": org_changes,
            "num_failed_steps": failed_steps,
            "avg_wait_time_min": avg_wait_time
        })

    return pd.DataFrame(process_metrics)

# è¯„åˆ†é€»è¾‘
def score_processes(metrics_df):
    norm_df = (metrics_df.drop(columns=['process_id']) - metrics_df.drop(columns=['process_id']).min()) / \
              (metrics_df.drop(columns=['process_id']).max() - metrics_df.drop(columns=['process_id']).min())
    norm_df.fillna(0, inplace=True)

    weights = {
        'total_duration_min': 0.35,
        'avg_activity_duration_min': 0.20,
        'num_activities': 0.10,
        'num_org_units': 0.15,
        'num_failed_steps': 0.10,
        'avg_wait_time_min': 0.10
    }

    score = 100 - (norm_df * pd.Series(weights)).sum(axis=1) * 100
    metrics_df["score"] = score.round(1)
    return metrics_df

st.subheader("ğŸ” åŸå§‹æµç¨‹æ—¥å¿—æ•°æ®")
st.dataframe(df, use_container_width=True)

metrics_df = compute_metrics(df)
scored_df = score_processes(metrics_df)

st.subheader("ğŸ“ˆ æµç¨‹æŒ‡æ ‡ + è‡ªåŠ¨è¯„åˆ†")
st.dataframe(scored_df, use_container_width=True)

st.subheader("ğŸ“Š æµç¨‹è¯„åˆ†é›·è¾¾å›¾")
fig = px.line_polar(scored_df, r='score', theta='process_id', line_close=True,
                    title="æµç¨‹è¯„åˆ†é›·è¾¾å›¾", markers=True)
st.plotly_chart(fig, use_container_width=True)

st.subheader("ğŸ“Š æµç¨‹æŒ‡æ ‡æŸ±çŠ¶å›¾")
fig_bar = px.bar(scored_df.sort_values("score", ascending=False), x='process_id', y='score',
                 title="æµç¨‹è¯„åˆ†æŸ±çŠ¶å›¾", text='score')
st.plotly_chart(fig_bar, use_container_width=True)

st.markdown("---")
st.markdown("ğŸ” å½“å‰æ”¯æŒä¸Šä¼ CSVæ–‡ä»¶è¿›è¡Œè¯„åˆ†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å†…ç½®æ¨¡æ‹Ÿæ•°æ®ã€‚å»ºè®®å­—æ®µåŒ…æ‹¬ï¼šæµç¨‹IDã€äº‹ä»¶IDã€äº‹ä»¶åç§°ã€äº‹ä»¶å¼€å§‹/ç»“æŸæ—¶é—´ã€æ‰§è¡Œäººã€æ‰§è¡Œæœºæ„ã€çŠ¶æ€ç­‰")
